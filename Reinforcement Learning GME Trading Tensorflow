# -*- coding: utf-8 -*-
"""Reinforcement Learning GME Trading Tensorflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16NgEOO4jNAbLBovHVWeEInB0gbNDN2Mw

# 0. Install and Import dependencies
"""

!pip install tensorflow-gpu==1.15.0 tensorflow==1.15.0 stable-baselines gym-anytrading gym

# Gym stuff
import gym
import gym_anytrading

# Stable baselines - rl stuff
from stable_baselines.common.vec_env import DummyVecEnv
from stable_baselines import A2C # Action Critic
from stable_baselines import DQN # Deep Q network

# Processing libraries
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

"""# 1. Bring in Marketwatch GME Data
https://www.marketwatch.com/investing/stock/gme/download-data?startDate=5/19/2020&endDate=05/19/2021


"""

!pip install gdown
!gdown  https://drive.google.com/uc?id=1qJAHj2UPWM6qxu7K3lxswj0qRuwxFItZ

"""Loading Data using Pandas"""

df = pd.read_csv('gme_data.csv')

df.head()

df.shape

df.dtypes

# Change datatype of Date from object to datetime as Gym only supports that
df['Date'] = pd.to_datetime(df['Date'])
df.dtypes

df.sort_values('Date', ascending=True, inplace=True)
df.head()

df.set_index('Date', inplace=True)
df.head()

env = gym.make('stocks-v0', df=df, frame_bound=(5,100), window_size=5)

# frame 0 - day 0 - day 4
# frame 1 - day 5 - day 9

print("Env information:")

print("> shape:", env.shape)

print("> df.shape:", env.df.shape)

print("> action [Buy | Sell]:", env.action_space)

print("> prices.shape:", env.prices.shape)

print("> signal_features.shape:", env.signal_features.shape)

print("> max_possible_profit:", env.max_possible_profit())

env.prices

"""# 2. Build Environment"""

env.action_space

state = env.reset()
while True:

    action = env.action_space.sample() # dummy
    n_state, reward, done, info = env.step(action)
    if done:
        print("info", info)
        break

plt.figure(figsize=(15,6))
plt.cla()

env.render_all() # Trades
plt.show()

"""# 2.1 Add Custom Indicators

## 2.1.1. Install and Import New Dependencies
"""

!pip install finta

from gym_anytrading.envs import StocksEnv
from finta import TA

"""## 2.1.2. Fix Volume Column"""

df['Volume'] = df['Volume'].apply(lambda x: float(x.replace(",", "")))

df.dtypes

"""## 2.1.3. Calculate SMA, RSI and OBV"""

df['SMA'] = TA.SMA(df, 12)
df['RSI'] = TA.RSI(df)
df['OBV'] = TA.OBV(df)
df

df.fillna(0, inplace=True)

df.head(15)

"""## 2.1.4. Create New Environments"""

def add_signals(env):
    start = env.frame_bound[0] - env.window_size # starting row of training data
    end = env.frame_bound[1] # ending row of training data
    prices = env.df.loc[:, 'Low'].to_numpy()[start:end]
    signal_features = env.df.loc[:, ['Low', 'Volume','SMA', 'RSI', 'OBV']].to_numpy()[start:end]
    return prices, signal_features

class MyCustomEnv(StocksEnv):
    _process_data = add_signals

env2 = MyCustomEnv(df=df, window_size=12, frame_bound=(12,50))

env2.signal_features

df.head()

"""# 3. Build Environment and Train"""

# Env without Indicators
env_maker = lambda: gym.make('stocks-v0', df=df, frame_bound=(5,100), window_size=5) # Python lambda to create multiple environment


env = DummyVecEnv([env_maker]) # Stable Baseline Class

model = DQN('MlpLstmPolicy', env, verbose=1)  # Environment of model [DQN, A2C]

# Multi Layer Perceptron - Neural Network
# LSTM - Long Short Term Memory

model.learn(total_timesteps=50000)

# Higher the expained_variance closer to one, better the model predict actions.

"""# 3.1 Add Custom Callbacks"""



"""# 4. Evaluation"""

# Env without Indicators
env = gym.make('stocks-v0', df=df, frame_bound=(90,100), window_size=5)

# # Env With More Indicators
# env = MyCustomEnv(df=df, window_size=12, frame_bound=(80,250))

obs = env.reset()

while True:
    obs = obs[np.newaxis, ...]
    action, _states = model.predict(obs)
    print("Predicted Action: ",action) # Sell=0 and Buy=1
    obs, rewards, done, info = env.step(action)
    if done:
        print("info", info)
        break

df.shape

# Train : Test = 80% : 20%
# Train : Val : Test = 60% : 20% : 20%

# 120th coulmn
# 130th column

env = gym.make('stocks-v0', df=df, frame_bound=(120,130), window_size=5)

obs = env.reset()
# (10,5,2)
# model.predict()

while True:
    print(obs.shape)

    # obs = obs[np.newaxis, ...]
    # print(obs.shape)
    obs = np.expand_dims(obs, axis=0)

    action, _states = model.predict(obs) #

    obs, rewards, done, info = env.step(action)

    if done:
        print("info", info)
        break



plt.figure(figsize=(15,6))
plt.cla()
env.render_all()
plt.show()

